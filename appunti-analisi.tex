\documentclass[11pt]{article}


\input{template/preamble}

\begin{document}

\title{Analisi 2}
\author{Guglielmo Bartelloni}

\maketitle
\tableofcontents
\newpage
\thispagestyle{empty}


\section{Lezione 1}

\subsection{Equazioni differenziali}

Le equazioni differenziali sono equazioni in cui l'incognita è un equazione insieme a qualche sua derivata.

\subsubsection{Equazioni differenziali ordinarie}

Noi vedremo quelle del primo ordine lineari e di secondo ordine con coefficienti costanti

Problema di Cauchy: problema con condizioni iniziali.


\defn{}{Una equazione di ordine n è una equazione del tipo:
\[
    F(x,y(x),y'(x),...,y^{(n-1)}(x),y^{(n)}(x))=0
\]
\[
    x \in I \subseteq \mathbb{R}
\]
dove l'incognita è la qualunque y(x). F è funzione di (n+2) variabili $x,y(x),y'(x)....$
}



L'\textbf{ordine} è dato dal massimo ordine di derivazione che compare.


Per esempio:
\[
    y'''+2y''+5y = e^x
\]
è di ordine 3

\defn{Soluzione (curva) integrale}{La soluzione di una EDO di ordine n sull'intervallo I \[
        (*) F(x,y(x),y'(x),...) = 0
\]
\[
    x \in I \subseteq \mathbb{R}
\]

$\varphi(x)$ che sia definita (almeno) in I e ivi derivabile fino all'ordine n per cui valga (*), ovvero:
\[
    F(x,\varphi(x),\varphi ' (x), ... ) = 0 
\]

$\forall x \in I$

Chiaramente cambia a seconda dell'intervallo
}

\defn{Integrale Generale}{Si chiama integrale \textbf{generale} di (*) in I l'insieme di tutte le soluzioni di (*) in I}


E' possibile definire un espressione piu' esplicita

\defn{Forma normale}{Un edo di ordine n si dice in forma normale se è in forma

    \[
        y^{(n)} = f(x,y(x),y'(x),....,y^{(n-1)}), x \in I
    \]
    
    Esempio:
    \[
        y'''=-5y'+sinx
    \]
    Quella sopra è un EDO di III ordine normale.
}

\defn{EDO di ordine n lineare}{Una EDO di ordine n si dice lineare se è nella forma
    \[
        a_n(x)y^{(n)}(x)+a_{n-1}(x)y^{(n-1)}+... + a_2(x)y''(x)+a_1(x)y'(x)+a_0y(x)=f(x),x \in I
    \]

    Dove le funzioni \[
        a_0(x),a_1(x),a_2(x),...,a_n(x),f(x)
    \]

    sono assegnate (continue) in I

    Esempio:
    \[
        xy''+5y = sin x
    \]

}


Quando $f(x)=0$ allora l'equazione si dice l'\textbf{omogenea associata} 


Nel nostro caso le equazioni di secondo ordine lineari saranno a \textbf{coefficienti costanti}


Vediamo come si risolve il problema della determinazione delle soluzioni di EDO lineari


\subsubsection{I ordine (n=1)}

\[
    F(x,y(x),y'(x))=0
\]

La considero in forma normale:
\[
    (1)\ y'(x)+a(x)y(x)=f(x), x \in [a,b]
\]

dove le funzioni $a(x)$ e $f(x)$ sono continue in $[a,b]$


Se $f(x)=0$ si ottiene omogenea associata:
\[
    (2)\ y'(x)+a(x)y(x)=0
\]

Come si determina l'integrale generale di (1)?

Il teorema che enunciamo vale per tutte le EDO lineari di ordine n

\teorema{}{L'integrale generale di (1) in $[a,b]$ è dato dalla somma dell'integrale generale dell'omogenea associata (2)
con un integrale particolare noto di (1)
\[
\int_{{}}^{{}} {gen} (1) = \int_{{}}^{{}} {gen(2)}  + \int_{{}}^{{}} {particolare} (1)
\]
}

\begin{proof}
    



Sia $y(x)$ una soluzione qualsiasi di (1) ($y(x)$ appartiene all'integrale generale di (1))
e sia $\bar y(x)$ una soluzione particolare (nota) di (1). Voglio far vedere è che la loro differenza è una soluzione qualsiasi di (2)

Dunque per ipotesi n ha che:
\[
    y'(x)+a(x)y(x) = f(x), \forall x \in [a,b]
\]

\[
    \bar y'(x) + a(x) \bar y(x) = f(x)
\]

Entrambe soddisfano la (1)

Sottraggo membro a membro le due:

\[
    y'(x)-\bar y'(x) + a(x)y(x) - a(x) \bar y(x) = f(x) - f(x)
\]

\[
    y'(x)-\bar y'(x) + a(x)[y(x) - \bar y(x)]=0
\]

Si può scrivere anche (le derivate raccolte):
\[
    [y(x)-\bar y(x)]' + a(x)[y(x) - \bar y(x)]=0
\]

E dunque  la funzione $y(x) - \bar y(x) = z(x)$ è soluzione di (2)
Quindi:
\[
    y(x) = \bar y(x) + z(x)
\]

Viceversa se $z(x)$ è una qualsiasi soluzione di (2) e $\bar y(x)$ è una soluzione particolare di (1) 
voglio mostrare che la loro somma è soluzione di (1)

Pongo:
\[
    y(x) = z(x) + \bar y(x)
\]

Devo mostrare che $y(x)$ verifica (1)

sapendo che:
\[
    z'(x) + a(x)z(x) = 0
\]

\[
    \bar y'(x) + a(x) \bar y(x) = f(x)
\]

\[
    y'(x) = (z(x) + \bar y(x) )' = z'(x) + \bar y'(x) =
    -a(x)z(x)-a(x)\bar y(x) + f(x) = -a(x) [z(x) + \bar y(x)] +f(x)
\]

E quindi ho dimostrato che:
\[
    y'(x) = -a(x)y(x) + f(x)
\]

\[
    y'(x) +  a(x)y(x) = f(x)
\]

\[
    y(x) = z(x) + \bar y(x)
\]

\end{proof}

\newpage

\section{Lezione 2}
\subsection{Facciamo vedere che il teorema precedente valeva anche per $n>1$}

Supponiamo che $u$ e $v$ siamo due soluzioni di (1), cioè che:

$Lu=f$ e $Lv=f$ su $I$

La differenza di queste diventano soluzione su $I=[a,b]$ dell'omogenea associata

Usando la propietà della linearità:

\[
    L(\lambda u+\mu v) = \lambda L u + \mu L v
\]

\[
    L(u-v) = Lu-Lv = f- f=0
\]

Se indichiamo con $V_0$ l'insieme di tutte le soluzioni dell'equazione omogenea associata ($Lw=0$ su $I=[a,b]$ e $V_0$ è l'insieme delle $w \in \mathbb{C}^n(I)$) e con $\bar u(t)$ una soluzione nota di (1)

\[
    u(x) = \bar u(x) +w(x)
\]

L'uguaglianza sopra, al variare di $w(x)$ in $V_0$ ci da tutte le soluzioni del problema di partenza. 

(Il problema quindi, diventa solo di studiare il problema omogeneo)

\subsection{Torniamo al I ordine}

Adesso ritorniamo al problema di I ordine (in forma normale):

\[
    (1)\ y'(x)+a(x)y(x)=f(x)
\]

dove $a()$ e $f()$ sono continue su $[a,b]$

\[
    (2)\ y'(x)+a(x)y(x)=0
\]

Secondo il teorema della prima lezione:

\[
    y(x)=z(x)+\bar y(x)
\]

Come si determina l'insieme di tutte le soluzioni (integrale generale) di (2), cioè:

\[
    (2)\ y'(x)+a(x)y(x)=0,x \in [a,b]
\]

Sia $A(x)$ una \textbf{primitiva} di $a(x)$:

\[
    A(x) = \int_{{}}^{{}} {a(x)} \: d{x} {}
\]

Moltiplichiamo i due membri della (2) per $e^{A(x)}$:

\[
    e^{A(x)} y'(x) + e ^{A(x)}a(x) y(x)=0, x \in [a,b]
\]

La posso scrivere anche (la derivata di $e ^{A(x)}y(x)$):

\[
    (e ^{A(x)} y(x))' = e ^{A(x)}a(x)y(x) + e ^{A(x)}y'(x)
\]

quindi (sempre chiaramente nell'intervallo $[a,b]$):

\[
    (e ^{A(x)}y(x))'=0
\]

Questo mi dice che:

\[
    e ^{A(x)}y(x) = costante=c \in \mathbb{R}
\]

porto dall'altra parte:

\[
    y(x) = c e ^{-A(x)}
\]

espandendo $A(x)$:

\[
    y(x) = c e ^{\int_{{}}^{{}} {a(x)} \: d{x} {}}
\]

posso considerare le soluzioni come:

\[
    y(x) = c z_0
\]

dove $z_0$ è una soluzione particolare di (2).

Infatti $e ^{-A(x)}$ è soluzione di (2)

\begin{proof}
    \[
    e ^{-A(x)} = -a(x) e ^{-A(x)}
\]

ovvero

\[
    (e ^{-A(x)})'+a(x) e ^{-A(x)}=0
\]
    
\end{proof}


\subsubsection{Determinazione dell'integrale particolare}

Sappiamo:

\[
    (1)\ y'(x)+a(x)y(x)=f(x)
\]

\[
    (2)\ y'(x)+a(x)y(x)=0
\]

Cerco l'integrale particolare ad \textbf{occhio} oppure uso il \textbf{metodo della variazione della costante}

\paragraph{Metodo della variazione della costante}

Cerco questa $c(x)$ in questa forma:

\[
    \bar y(x) = c(x) e ^{-A(x)}
\]

Ovviamente la cerco dopo che so che $\bar y(x)$ è soluzione del problema.

\begin{proof}
    Poichè $\bar y(x)$ è soluzione di (1) si ha che $\bar y'(x)+a(x) \bar y(x)=f(x)$ da cui sostituendo $\bar y(x) = c(x) e ^{-A(x)}$:

    \[
        (c(x) e ^{-A(x)})'+ a(x) c(x) e ^{-A(x)} = f(x)
    \]
    
    Deriviamo:

    \[
        c'(x) e ^{-A(x)} - c(x) a(x) e ^{-A(x)} + a(x) c(x) e ^{-A(x)}= f(x)
    \]
    
    semplifico

    \[
        c'(x) e ^{-A(x)} = f(x)
    \]

    \[
        c'(x) = f(x) e ^{A(x)} \rightarrow c(x) = \int_{{}}^{{}} {f(x) e ^{A(x)}} \: d{} {}
    \]

    e dunque:

    \[
        \bar y(x) = e ^{-A(x)} \int_{{}}^{{}} {f(x) e ^{A(x)}} \: d{x} {}
    \]

    Cioè l'integrale particolare
\end{proof}

Se metto tutto insieme l'integrale generale diventa:

\[
    y(x) = c e ^{-A(x)} + e ^{-A(x)} \int_{{}}^{{}} {f(x) e ^{A(x)}} \: d{x} {}
\]

\subsubsection{Osservazioni sulla formula}

$A(x)$ è \textbf{una} primitiva di $a(x)$ scelta una volta per tutte.

\textbf{Non} occorre mettere una costante arbitraria (ovvero considerare come $A(x) + K,K \in \mathbb{R}$ ) poiche l'integrale generale non cambia

\textbf{Non} serve neanche nell'integrale perchè verrebbe buttato dentro $c$ dell'integrale generale


\subsubsection{Esempi}

\[
    y'(x) = 5y(x) + e ^{x}
\]

in questo caso $a(x) = -5$

\[
    A(x) = - \int_{{}}^{{}} {5} \: d{x} {}=-5x
\]

Quindi: 

\[
    e ^{-A(x)}=e ^{5x}
\]

\[
    y(x) = c e ^{5x} + e ^{5x} \int_{{}}^{{}} {e^x e ^{-5x}} \: d{x} {} = c e ^{5x} + e ^{5x} \int_{{}}^{{}} {e ^{-4x}} \: d{} {} = c e ^{5x} + e ^{5x} (\frac{1}{4} e ^{-4x}) = c e ^{5x} - \frac{1}{4} e ^{x}
\]

Esercizio per casa:

\[
    u' + \frac{u}{t} = e ^{t}
\]

\newpage

\section{Lezione 3}

Solitamente si suppongono delle condizioni iniziali nel risolvere le equazioni differenziali (problema di Cauchy).

\begin{equation}
    \begin{cases}
      y'(x)+a(x)y(x)=f(x)\\
      y(x_0)=y_0
    \end{cases}\,.
\end{equation}

Praticamente gli integrali della formula generale diventano definiti tra $x_0$ e $x$.

Quindi:

\[
    y(x) = ce ^{-A(x)}+ e ^{-A(x)} \int_{{}}^{{}} {e ^{A(x)}f(x)} \: d{x} {} = c e ^{-\int_{{x_0}}^{{x}} {a(t)} \: d{t} {}}+e ^{- \int_{{x_0}}^{{x}} {a(t)} \: d{t} {}}\int_{{x_0}}^{{x}} {e ^{\int_{{x_0}}^{{s}} {a(t)} \: d{t} {}}f(s)} \: d{s} {}\\
\]

\[
    y(x_0)=y_0=c
\]


Voglio trovare la soluzione generale in questo caso, parto dall'omogenea:

\[
    y'+x(x)y(x) = 0
\]

\[
    e ^{\int_{{x_0}}^{{x}} {a(x)} \: d{t} {}} = e ^{A(x)}
\]


\subsection{Il problema di Cauchy}

Quindi introduciamo il problema di Cauchy:

\begin{equation}
    \begin{cases}
      y'+a(x)y = f(x)\\
      y(x_0) = y_0
    \end{cases}\,.
\end{equation}

dove $x \in I = [a,b]$ e $x_0 \in I$

con le ipotesi fatte ($a(x)$ e $f(x)$ continue in I) ha una e una sola soluzione (SOLUZIONE UNICA)

con l'espressione esplicita determinata.

\textbf{Esempio 1}

Determinare la soluzione del problema di Cauchy:

\begin{equation}
    \begin{cases}
      y'(x)=5y(x) + e ^{x}\\
      y(0)=0
    \end{cases}\,.
\end{equation}

\[
    A(x)=\int_{{0}}^{{x}} {a(t)} \: d{t} {}= - \int_{{0}}^{{x}} {5} \: d{t} {}= -5x
\]

\[
    y(x)=0e ^{5x} + e ^{5x}\int_{{0}}^{{x}} {e ^{-5t}e ^{t}} \: d{t} {}=
\]

\[
    = e ^{5x} \Eval{[ -\frac{1}{4} e ^{-4t}]}{0}{x} = e ^{5x}(-\frac{1}{4} e ^{-4x}+\frac{1}{4})= - \frac{1}{4} e ^{x}+ \frac{1}{4} e ^{5x}
\]

\textbf{Esempio 2}

Determinare l'integrale generale della EDO:

\[
    y'+\frac{1}{\sqrt{x}} y=1
\]

e trovare le eventuali soluzioni tali che:

\[
    \lim_{x \to \infty} y(x) = +\infty
\]

Soluzione: 

l'equazione è definita per ogni $x>0$

\[
    a(x) = \frac{1}{\sqrt{x}} 
\]

\[
    A(x) = \int_{{}}^{{}} {\frac{1}{\sqrt{x}} } \: d{x} {}
\]


L'integrale generale:

\[
    y(x) = c e ^{-\int_{{}}^{{}} {\frac{1}{\sqrt{x}} } \: d{x} {}}+ e ^{-\int_{{}}^{{}} {\frac{1}{\sqrt{x}} } \: d{x} {}}( \int_{{}}^{{}} {e ^{\int_{{\frac{1}{\sqrt{x}} }}^{{}} {} \: d{x} {+1}}} \: d{x} {})=
\]

\[
    =e ^{2 \sqrt{x}} (e+ \int_{{}}^{{}} {e ^{2\sqrt{x}}} \: d{x} {})
\]

Risolvo l'integrale pongo $t = 2 \sqrt{x}$ quindi $ dt = \frac{1}{\sqrt{x}}dx \rightarrow dx = \frac{t}{2} dt $:

\[
    \int_{{}}^{{}} {e ^{2 \sqrt{x}}} \: d{x} {}= \int_{{}}^{{}} {e ^{t}\frac{t}{2} } \: d{t} {} = e ^{x}\frac{t}{2} - \int_{{}}^{{}} {e ^{t}\frac{1}{2} } \: d{t} {}=
\]

\[
    = e ^{t} \frac{t}{2} - \frac{1}{2 e ^{t}} 
    \overset{\text{risostituisco}}{=} e ^{2 \sqrt{x}} \frac{2 \sqrt{x}}{2} - \frac{1}{2} e ^{2 \sqrt{x}}
\]

Ora riscrivo l'integrale generale:

\[
    y(x) = e ^{-2 \sqrt{x}}[ c + e ^{2 \sqrt{x}}(\sqrt{x}- \frac{1}{2} )]= c e ^{-2 \sqrt{x}}+ \sqrt{x} - \frac{1}{2} 
\]

Adesso soddisfo la richiesta (quali sono le soluzioni che vanno all'infinito)

\[
    \lim_{x \to \infty} c ^{-2 \sqrt{x}} + \sqrt{x} - \frac{1}{2} = +\infty
\]

questo vale per $\forall c \in \mathbb{R}$


\textbf{Esempio 3}

\begin{equation}
    \begin{cases}
      y' + \frac{2y}{x} = \frac{1}{2} \\
      y(-1)=2
    \end{cases}\,.
\end{equation}

Considero l'intervallo dove sta il $x_0=-1$ quindi $(-\infty,0)$

\[
    A(x) = \int_{{-1}}^{{x}} {\frac{1}{t} } \: d{t} {} = \Eval{[2 log|t|]}{-1}{x} = 2 log|x| - 2 log|-1| = 2 log|x| = 
\]

per via dell'intervallo il valore assoluto viene preso col meno:

\[
    =2 log(-x)  
\]

quindi l'integrale generale:

\[
    y(x) = 2 e ^{-2log(-x)}+ e^{-2log(-x)}(\int_{{-1}}^{{x}} {e ^{2log(-t)}\frac{1}{t ^{2}} } \: d{t} {})=
\]

uso la proprietà dei logaritmi:

\[
    = 2 e ^{log \frac{1}{x ^{2}} }+ e ^{log \frac{1}{x ^{2}} }\int_{{-1}}^{{x}} {e ^{log t ^{2}}} \: d{t} {}= \frac{2}{x ^{2}} + \frac{1}{x ^{2}} \int_{{-1}}^{{x}} {1} \: d{t} {} = \frac{2}{x ^{2}} + \frac{1}{x ^{2}} \Eval{[t]}{-1}{x} 
     = \frac{2 }{x ^{2}} + \frac{1}{x ^{2}} (x+1)
\]



\newpage

\section{Lezione 4}

\subsection{Edo a variabili separabili}

Una edo si dice a variabili separabili se è della forma:

\[
    y'(x) = f(x) g(y(x))
\]

Parte che dipende da y viene moltiplicata a quella che dipende da x.

Dove le funzioni f e g sono continue nei loro domini di definizione

Il procedimento per risolverle è il seguente:

\begin{enumerate}
    \item Si cercano le soluzioni costanti $g(y)=0$ (cioè gli zeri)

        Si determinano gli eventuali $\bar y$ reali t.c. $g(\bar y)$

        $y(x)= \bar y$ sono soluzioni singolari del problema 
        
    \item Se $y \neq \bar y$ si procede separando le variabili, ovvero dividiamo per $g(y)$
\end{enumerate}

E quindi alla fine abbiamo:

\[
    \frac{y'(x)}{g(y(x))} = f(x) \overset{\text{integro rispetto ad x}}{=} \int_{{}}^{{}} {\frac{y'(x)}{g(y(x))} } \: d{x} {} = \int_{{}}^{{}} {f(x)} \: d{x} {}
\]

Uso la sostituzione $y = y(x)$ e $dy = y'(x) dx$:

\[
    \int_{{}}^{{}} {\frac{1}{g(y)} } \: d{y} = {\int_{{}}^{{}} {f(x)} \: d{x} {}}
\]

Chiamate $G$ e $F$ una primitiva di $\frac{1}{g} $ e di $f$ rispettivamente:

\[
    G(y(x)) = F(x) + c
\]

Applico la funzione inversa di $G$ ad entrambi i membri per scrivere esplicitamente la soluzione:

\[
    y(x) = G ^{-1} (F(x) + c)
\]

\textbf{Esempio}

Determinare tutte le soluzioni dell'equazione differenziale:

E' non lineare

\[
    y'(x) = (1-y)(2-y)x
\]

Le prime due parentesi sono $g(y)$ il resto $f(x)$

\begin{enumerate}
    \item Trovare le soluzioni costanti

        Pongo $y'(x) = 0 $:

        \[
            (1-y)(2-y) = 0
        \]

        quindi $y=1$ e $y=2$

    \item Cerchiamo le altre soluzioni dividendo per $g(y)$

        \[
            \frac{y'(x)}{1-y(x)(2-y(x))} = x
        \]

        Quindi integro:

        \[
            \int_{{}}^{{}} {\frac{1}{(1-y)(2-y)} } \: d{y} {}= \int_{{}}^{{}} {x} \: d{x} {}
        \]

        Uso i fratti semplici per risolvere il primo membro:

        \[
            \frac{A}{1-y} + \frac{B}{2-y} = \frac{1}{(1-y)(2-y)} 
        \]

        \[
            A(2-y)+B(1-y) = 1
        \]

        \[
            (-A -B) y +2A + B = 1
        \]

        \begin{equation}
            \begin{cases}
              -A -B = 0\\
              2A+B= 1
            \end{cases}\,.
        \end{equation}

        $A=1$ e $B=1$
        
        Quindi:

        \[
            \int_{{}}^{{}} {\frac{1}{1-y}} \: d{y} {}- \int_{{}}^{{}} {\frac{1}{2-y} } \: d{y} {} = \int_{{}}^{{}} {x} \: d{x} {}
        \]

        \[
            -log|1-y| + log|2-y| = \frac{x ^{2}}{2} +c
        \]

        \[
            log|\frac{2-y}{1-y} | = \frac{x ^{2}}{2} +c
        \]

        Adesso devo esplicitare per $y$ quindi passo agli esponenziali:

        \[
            |\frac{2-y}{1-y} | = e^{(\frac{x ^{2}}{2} +c)}
        \]

        \[
            |\frac{2-y}{1-y} | = e^{(\frac{x ^{2}}{2})} e ^{c} = c_1 e ^{\frac{x ^{2}}{2} } >0
        \]

        Tolgo il valore assoluto:

        \[
            \frac{2-y}{1-y} = \pm c_1 e ^{\frac{x ^{2}}{2} }\overset{\text{usando un'altra costante}}{=}c_2 e ^{\frac{x ^{2}}{2} }
        \]

        \[
            \frac{2-y}{1-y} = c_2 e ^{\frac{x ^{2}}{2} }
        \]

        Con $c_2 \in \mathbb{R}$

        Noi vogliamo trovare la $y(x)$ (per semplicita pongo $c_2 = c$):

        \[
            \frac{2-y}{1-y} = c e ^{\frac{x ^{2}}{2} }
        \]

        Porto di la il denominatore:

        \[
            2-y = c ^{\frac{x ^{2}}{2} } (1-y)
        \]

        Porto di la le cose:

        \[
            (c e ^{\frac{x ^{2}}{2} })y = c ^{\frac{x ^{2}}{2} }-2
        \]

        E quindi le due soluzioni (quella costante e quella non) sono:

        \begin{equation}
            \begin{cases}
            y(x) = \frac{c e ^{\frac{x ^{2}}{2} }-2}{c e ^{\frac{x ^{2}}{2} }-1}   \\
            y=1
            \end{cases}\,.
        \end{equation}

\end{enumerate}

\textbf{Esercizio Problema di Cauchy}

Risolviamo ora il problema:

\begin{equation}
    \begin{cases}
      y'=(1-y)(2-y)x\\
      y(0)=3
    \end{cases}\,.
\end{equation}

e decidiamo qual è il piu ampio intervallo su cui è definita la soluzione

Avendo già risolto la EDO imponiamo la condizione $y(0) = 3$:

\[
    y(0) = \frac{c-2}{c-1} = 3
\]

\[
    c-2 = 3c -3
\]

\[
    c = \frac{1}{2} 
\]

La soluzione del problema è quindi (sostituisco la c trovata all'equazione):

\[
    y(x) = \frac{\frac{1}{2} e ^{\frac{x ^{2}}{2} }-2}{\frac{1}{2} e ^{\frac{x ^{2}}{2} }-1} 
\]

\[
    y(x) = \frac{ e ^{\frac{x ^{2}}{2} }-4}{ e ^{\frac{x ^{2}}{2} }-2} 
\]

La soluzione è definita nel piu ampio intervallo contenente $x_0 = 0 $ ( per cui l'esressione ha senso) nel nostro caso il denominatore $\neq 0$

\[
    e ^{\frac{x ^{2}}{2} } - 2 \neq 0
\]

\[
    e ^{\frac{x ^{2}}{2} }  \neq 2
\]

\[
    x ^{2} \neq 2 log 2
\]

\[
    x \neq \pm \sqrt{2 log2}
\]

Quindi l'intervallo piu ampio è quello che contiene zero ed è compreso tra le regole che abbiamo appena trovato:

\[
    0 \in (-\sqrt{2log2},+\sqrt{2log2}) 
\]

Osserviamo che la soluzione:

\[
    y(x) = \frac{ e ^{\frac{x ^{2}}{2} }-4}{ e ^{\frac{x ^{2}}{2} }-2} 
\]

è definita $\forall x \in \mathbb{R}$ con $x \neq \pm \sqrt{2log2}$


Il motivo per cui la soluzione del problema di Cauchy è definita su un intervallo si capisce bene se si pensa al significato fisico del nostro problema:

\begin{equation}
    \begin{cases}
        \text{x tempo}\\
        \text{y(x) evoluzione del sistema}\\
        \text{condizione iniziale}
    \end{cases}\,.
\end{equation}

Se partendo dall'istante iniziale ($x_0$) e procedendo in avanti o a ritroso nel tempo troviamo un istante per cui il sistema non esiste (nel caso di prima $\pm \sqrt{2log2}$) la $y(x)$ non esiste piu, non ha senso domandarsi che cosa succede oltre quell'istante

Se lo vedo dal punto di vista matematico se accettassimo soluzioni definite su intervalli disgiunti non avremmo piu l'unicità della soluzione (ce ne sarebbero 3 nel nostro caso e non una come volevo) perche avremmo rami distinti della funzione $y(x)$ definiti su intervalli disgiunti che non si raccordano tra di loro, dunque la condizione iniziale $y(x_0) = y_0$ non determina i valore della funzione $y(x)$ negli intervalli che non contentgono l'istante iniziale $x_0$

\begin{itemize}
    \item \textbf{ Soluzione in piccolo (locale) } (è definita in un intorno di $x_0$)
    \item \textbf{ Soluzione in grande (globale) } (è definita in tutto l'intervallo)
\end{itemize}

\textbf{Esercizio per casa}

\begin{equation}
    \begin{cases}
      y'(x) = xy(x)+2x\\
      y(0) = 1
    \end{cases}\,.
\end{equation}

\textbf{Soluzione} 

Raccolgo: 

\[
    y'(x)=x(y+2)
\]

Trovo le soluzioni stazionarie:

\[
    y+2=0
\]

\[
    y=-2
\]

Trovo le altre:

\[
    \int_{}^{} {\frac{1}{y+2}} \: dy = \int_{}^{} {x} \: dx 
\]

\[
    y+2 = c e ^{ \frac{x^{2}}{2}}+c
\]

Impongo le condizioni di Cauchy e trovo c sostituendo:

\[
    y=3e ^{ \frac{x^{2}}{2}}-2
\]

Quindi la soluzione completa e':

    \begin{equation}
        \begin{cases}
            y=-2\\
            y=3e ^{ \frac{x^{2}}{2}}-2
        \end{cases}\,.
    \end{equation}



\subsection{EDO lineari del II ordine}

\[
    a_2(x)y''(x) + a_1(x) y'(x) + a_0(x) y(x) = f(x)
\]

con $a_0(),a_1(),a_2(),f()$ continue in I

In forma normale:

\[
    y''(x) + a(x) y'(x) + b(x)y(x) = f(x)
\]

se pongo $f(x) = 0$ ho la omogenea associata (2)

le sue soluzioni sono linearmente indipendenti

Se abbiamo due soluzioni $y_1$ e $y_2$ di:

\[
    a_2(x)y''(x) + a_1(x) y'(x) + a_0(x) y(x) = 0
\]

Poniamo:

\[
    y(x) = c_1 y_1(x) + c_2 y_2(x)
\]

io so che le soluzioni soddisfano l'equazione (per definizione):

\[
    a_2(x)y_1''(x) + a_1(x) y_1'(x) + a_0(x) y_1(x) = 0
\]

\[
    a_2(x)y_2''(x) + a_1(x) y_2'(x) + a_0(x) y_2(x) = 0
\]

adesso:

\[
    y(x) = c_1 y_1(x) + c_2 y_2(x)
\]

Derivo due volte:

\[
    y'(x) = c_1 y_1'(x) + c_2 y_2'(x)
\]

\[
    y''(x) = c_1 y_1''(x) + c_2 y_2''(x)
\]

\[
    a_2(x) [ c_1y_1''(x) + c_2 y_2 ''(x) ] + a_1(x)[ c_1 y_1'(x) + c_2 y_2'(x) ] + a_0(x) [ c_1 y_1(x) + c_2 y_2(x)]= 
\]

\[
    = c_1[a_2(x) y_1''(x) + a_1(x) y_1'(x) + a_0(x) y_1(x)] + c_2 [a_2(x) y_2''(x) + a_1(x) y_2'(x) + a_0(x) y_2(x)] \overset{\text{dato che è soluzione}}{=} 0
\]

\subsection{Lineare indipendenza}


$y_1(x)$ e $y_2(x)$ sono linearmente indipendenti su I se:

\[
    c_1y_1(x) +c_2y_2(x) = 0 \Leftrightarrow c_1=c_2=0
\]

\textbf{Esercizi per Casa} 

\textbf{Esercizio 1}

\[
    y(x) = ce ^{x^{2}-x}+e ^{x^{2}-x}\int_{}^{} {xe ^{x}} \: dx = c e ^{x^{2}-x}+xe ^{x^{2}}-e ^{x^{2}}
\]

Ponendo le condizioni di Cauchy:

\[
    y(0)=2
\]

La soluzione e':

\[
    2 e ^{x^{2}-x}+xe ^{x^{2}}-e ^{x^{2}}
\]

\textbf{Esercizio 2} 

\[
    y'=\sqrt[3]{x}y^{2}
\]

Una soluzione e':

\[
    y=0
\]

Le altre le trovo facendo l'integrale di:

\[
    \int_{}^{} { \frac{1}{y^{2}}} \: dy = \int_{}^{} {\sqrt[3]{x}} \: dx  
\]

quindi $y(x)$:

\[
    y(x)  = \frac{4}{3 \sqrt[3]{x^{4}}+c}
\]

impongo le condizioni e trovo c:

\[
    y(0) = \frac{-4}{0+c} = 2
\]

quindi:

\[
    c = -2
\]

ergo il la soluzione e':

\[
    y(x) = -\frac{4}{3 \sqrt[3]{x^{4}} - 2}
\]

il denominatore deve essere $\neq 0$:

\[
3 \sqrt[3]{x^{4}} - 2 \neq 0
\]

quindi:

\[
    x \neq (\frac{2}{3}^{ \frac{3}{4}})
\]

Il piu ampio intervallo e':

\[
    0 \in (-\infty, \frac{2}{3}^{ \frac{3}{4}}) 
\]

\newpage

\section{Lezione 5}

Ritorniamo alle equazione del secondo ordine.

\[
    a_2(x)y''(x) + a_1(x) y'(x) + a_0(x) y(x) = f(x)
\]

con $a_0(),a_1(),a_2(),f()$ continue in I $ \in  [a,b]$

ci conentriamo nel caso in cui le $a$ sono costanti (coefficienti costanti).

L'altra volta abbiamo dimostrato che se abbiamo due soluzioni $y_1$ e $y_2$ esse sono linearmente indipendenti cioe' il determinante della matrice di $y_1(x)y_2'(x)-y_2(x)y_2'(x)$ e' diverso da 0 (determinante Wronskiano)

Se quindi l'equazione ha coefficienti costanti diventa:

\[
    ay''(x)+by'(x)+cy(x) = f(x)
\]

con $a,b,c \in \mathbb{R}$ con $a \neq 0$ se no non sarebbe di ordine II, $f(x)$ e' continua in I

Adesso associamo il problema omogeneo:

\[
    ay''(x) + by'(x) + cy(x) = 0
\]

\textbf{Numeri complessi} 

Qui dobbiamo introdurre i numeri complessi perche ci servono per la soluzione, di solito questi sono formati da una parte reale e una parte immaginaria:

\[
    z = \alpha + i\beta
\]

$z$ puo essere scritto come coppia $(\alpha,\beta)$ ad $i$ assegno $i=\sqrt{-1}$

Tornando a noi vediamo il caso in cui $b=c=0$

\[
    ay''(x) = 0
\]

in I ed in particolare:

\[
    y''(x) = 0, \forall x \in I
\]

\[
    y'(x) = c, c \in \mathbb{R}
\]

\[
    y(x) = c_1x+c_0,c_1,c_0 \in \mathbb{R}
\]

Questo caso e' facile. Se invece $b$ e $c$ non sono contemoporaneamente nulli, devo considerare la sequente eq. algebrica di secondo grado:

\[
    p(\lambda) = a \lambda^{2}+b \lambda + c =0
\]

La sua equazione associata a (2):

\[
    p(\lambda) =0 \Leftrightarrow  a \lambda^{2}+b \lambda + c =0, in\ \mathbb{C}
\]

\teorema{Teorema fondamentale dell'algebra}{
    L'equazione di II in $\mathbb{C}$ 

    \[
   a \lambda^{2}+b \lambda + c =0, in\ \mathbb{C}
    \]

    ha sempre due soluzioni in $\mathbb{C}$

}


\textbf{Proposizione} 

$y(x) = e ^{\lambda x}$ e' soluzione di (2) $\Leftrightarrow $ $\lambda$ e' soluzione (radice) di $p(\lambda)=0$ dell'equazione caratteristica associata a (2)

Indico con $Ly$ l'equazione $Ly= ay''+by'+cy$

\begin{proof}
    y e' soluzione di (2) $\Leftrightarrow$ $Ly=0$ 

    Se considero $y(x) = e ^{\lambda x}$ 

    Devo dimostrare che:

    \[
        L(e ^{\lambda x}) = 0 \Leftrightarrow  p(\lambda) = 0
    \]

    Sostituisco ad $x$ $e ^{\lambda x}$:

    \[
        L(e ^{\lambda x}) = a( e^{\lambda x})'' + b( e ^{\lambda x})' + c(e ^{\lambda x}) =
    \]

    \[
        =a \lambda ^{2} e ^{\lambda x} + b \lambda e ^{\lambda x} + c e^{\lambda x}= e ^{\lambda x}(a \lambda ^{2}+ b \lambda+ c)
    \]

    dunque

    \[
        L( e ^{\lambda x}) = 0 \Leftrightarrow a \lambda ^{2}+ b \lambda +c = 0 
    \]
           
\end{proof}


Adesso che ho dimostrato il mio problema e' trovare le radici $p(\lambda) =0$ ($a \lambda ^{2} + b \lambda + c$):

Di solito le soluzioni di secondo grado si scrivono

\[
    \lambda_{1,2} = \frac{-b \pm \sqrt{b ^{2}-4 ac}}{2a}
\]
   
Le soluzioni $\lambda_1$ e $\lambda_2$ sono soluzioni di ((2) $e ^{\lambda_1x}$ e $e ^{\lambda_2x}$)

Distinguiamo tre casi per le soluzioni:

\begin{enumerate}
    \item soluzioni reali e distinte ($\Delta >0$)
    \item soluzioni reali e coincidenti ($\Delta = 0$)
    \item soluzioni complesse coniugate ($ \Delta <0$)
\end{enumerate}

1) $y_1(x) = e ^{\lambda_1x}$ e $y_2(x) = e ^{\lambda_2x}$ con $\lambda_1 e \lambda_2$ $\in \mathbb{R}$ con $\lambda_1 \neq \lambda_2$


2) $y_1(x) = e ^{\lambda x}$ e $y_2(x) = xe ^{\lambda x}$ con $\lambda = - \frac{b}{2a}=\lambda_1=\lambda_2$ $\in \mathbb{R}$ 

3) $y_1(x) = e ^{\alpha x} cos \beta x$ e $y_2(x) = e ^{\alpha x} sin \beta x$ 

questo caso corrisponde a soluzioni complesse coniugate  

\[
    \lambda_1 = \alpha- i \beta \in \mathbb{C} 
\]

\[
    \lambda_2 = \alpha+ i \beta \in \mathbb{C} 
\]

\[
    \lambda = \frac{-b \pm \sqrt{-(4ac-b^{2})}}{2a} = \frac{-b \pm \sqrt{-1(4ac - b^{2})}}{2a} \overset{\text{perche i} = \sqrt{-1}}{=} \frac{-b \pm  \sqrt{4ac -b^{2}}i}{2a} = \alpha \pm i \beta
\]

dove $\alpha = -\frac{b}{2a}$ e $\beta = \frac{\sqrt{4ac - b^{2}}}{2a} >0$


\teorema{}{L'integrale generale dell'equazione omogenea $a y''+by'+c=0$ e' dato da:

    \[
        c_1 y_1(x) + c_2 y_2(x)
    \]

    al variare di $c_1,c_2 \in \mathbb{R}$ dove $y_1(x)$ e $y_2(x)$ sono definite come sopra
}

\begin{proof}
       1) $b^{2}-4ac >0$ con $\lambda_1,\lambda_2$ soluzioni dell'equazioni di $p(\lambda)=0$    

       scrivo la wronskiana di $y_1,y_2$:
       \[
        \begin{bmatrix}
            
        e ^{\lambda_1 x} & e ^{\lambda_2 x} \\
        \lambda_1e ^{\lambda_1 x} & \lambda_2e ^{\lambda_2 x} \\
        
        \end{bmatrix}
       \]
        che e' diverso da zero quindi le soluzioni sono linearemente indipendenti

        sia ora $y(x)$ una soluzione di (2):

        \[
            y(x) = e ^{\lambda_1 x}u(x)
        \]

        io devo determinare $u(x)$ per poi dimostrare che $y(x) = c_1e ^{\lambda_1 x}+c_2 e^{\lambda_2 x}$

        Poiche $y(x) = e ^{\lambda_1 x}u(x)$ e' soluzione di (2) si ha derivando e sostituendo:

        \[
            a( e ^{\lambda_1 x} u(x))'' + b(e ^{\lambda_1 x}u(x))'+ c e ^{\lambda_1 x}u(x) =0
        \]

        \[
            a(\lambda_1 e ^{\lambda_1 x} u(x)+ e ^{\lambda_1 x}u'(x))' + b(\lambda_1e ^{\lambda_1 x}u(x) + e ^{\lambda_1 x}u'(x))+ c e ^{\lambda_1 x}u(x) =0
        \]

        \[
            e ^{(\lambda_1 x}[a \lambda_1 ^{2} + b \lambda_1+c)u(x)+\underbrace{(au''(x)+(2a \lambda_1 + b)u'(x))}_\text{impongo che sia zero}]=0
        \]

        estraggo solo l'ultima parentesi e impongo che sia uguale a zero perche il resto e' gia zero

        \[
            au''(x) + (2a \lambda_1 + b) u'(x) = 0
        \]

        divido per a:

        \[
            u''(x) +(2 \lambda_1 + \frac{b}{a}) u'(x) = 0
        \]

        sapendo che:

        \[
            a \lambda^{2} + b \lambda + c =0
        \]

        \[
             \lambda^{2} + \frac{b}{a} \lambda + \frac{c}{a} =0
        \]

        \[
            \lambda_1 + \lambda_2 = -\frac{b}{a}
        \]

        \[
            \lambda_1  \lambda_2 = \frac{c}{a}
        \]

        \[
            u''(x) + (2 \lambda_1 - \lambda_1 - \lambda_2)u'(x) = 0
        \]

        il meno per comodita:

        \[
            u''(x) - (\lambda_1 - \lambda_2)u'(x) = 0
        \]

        se adesso chiamo $u'(x)=v(x)$ e $v''(x) = u'(x)$ l'equazione diventa:

        \[
            v' -kv = 0
        \]

        Risolvendo 

        \[
            v(x) = ce ^{kx}
        \]

        \[
            v(x) = c e^{(\lambda_2 - \lambda_1)x}
        \]

        Risostitendo:

        \[
            u'(x) = c e ^{(\lambda_2- \lambda_1)x}
        \]

        Integrando:

        \[
            u(x)  = c_1 e ^{(\lambda_2 - \lambda_1)x}+c_2
        \]
        
        la nostra $y(x)$ diventa:

        \[
            y(x) = e ^{\lambda_1 x}u(x) = e ^{\lambda_1 x}( c_1 e ^{(\lambda_2 - \lambda_1)x}+c_2) = c_1 e ^{\lambda_2 x}+ c_2 e ^{\lambda_1 x}
        \]


\end{proof}

Adesso voglio per il caso 2)

\[
    \lambda_1 = \lambda_2 = \lambda = -\frac{b}{2a} \in \mathbb{R}
\]
   
\[
    p(\lambda) =0 \Leftrightarrow e ^{\lambda x} \text{e' soluzione di (2)}
\]

sia quindi $y(x)$ una soluzione di (2) che scriviamo come:

\[
    y(x) = e ^{\lambda x}u(x) 
\]

Come prima si ottiene:

\[
    a(e ^{\lambda x}u(x) )''+ b(e ^{\lambda x}u(x))' + c e ^{\lambda x}u(x)=0
\]

\[
    \overbrace{e ^{\lambda x}}^{>0}(a u''(x) + \underbrace{(a \lambda^{2}+b \lambda +c )}_\text{=0} u(x) + (2a \lambda+b)u'(x))=0
\]

estraggo la parte che impongo a zero:

\[
    au''(x) + (2a \lambda+b)u'(x) = 0
\]

divido per a:

\[
 u''(x) + (2 \lambda + \frac{b}{a})u'(x) = 0
\]

sapendo che $-\frac{b}{a} = 2 \lambda$ :

\[
 u''(x) + (\cancel{2 \lambda} + \cancel{\frac{b}{a}})u'(x) = 0
\]


\[
    u'(x) = c_1
\]

\[
    u(x) = c_1 x +c_2
\]

e quindi ho la soluzione:

\[
    y(x) = e ^{\lambda x}(c_1x+c_2) = c_1x e^{\lambda x}+ c_2 e ^{\lambda x}
\]

\newpage

\section{Lezione 6}

\subsection{Determinazione della soluzione particolare per EDO II ordine}


Dobbiamo vedere ora come si determina la soluzione particolare.

\[
    (1)\ ay''+by'+cy = f(x)\ \in I=[a,b]
\]

\[
    (2)\ ay''+by'+cy = 0\ \in I=[a,b]
\]

\[
    y(t) = c_1 y_1(x) + c_2 y_2(x) + \bar{y} (x)
\]

La $\bar{y}$ e' la soluzione particolare, ci sono due modi:

\begin{itemize}
    \item Si procede ad occhio, per similitudine guardando l'espressione di $f(x)$
    \item Si usa il metodo di variazione delle costanti

        \[
            \bar{y} = c_1(x) y_1(x) + c_2(x) y_2(x)
        \]

        dove ${y_1(x),y_2(x)}$ soluzioni linearmente indipendenti di (2) con $c_1(x),c_2(x)$ funzioni di classe $\mathbb{C}^{2}(I)$ da determinare.
\end{itemize}

Vediamo come fare con quest'ultimo metodo.

Poiche $\bar{y} (x)$ e' soluzione di (1) allora  $a\bar{y} ''+b \bar{y}'+c\bar{y}=f$

\[
    \bar{y} (x) = c_1(x) y_1(x) + c_2(x) y_2(x)
\]

\[
\bar{y} '(x) = c_1'(x) y_1(x) + c_1 y_1'(x) + c_2'(x) y_2(x) + c_2(x) y_2'(x)
\]

Adesso impongo che $c_1'(x) y_1(x) + c_2'(x) y_2(x) = 0$:

\[
    \bar{y} '(x) = c_1(x) y_1'(x) +c_2(x) y_2'(x)
\]

\[
    \bar{y} ''(x) = c_1'(x) y_1'(x) + c_1(x) y_1''(x) + c_2'(x) y_2'(x) + c_2(x) y_2''(x)
\]

sapendo che $a \bar{y} ''+ b \bar{y} ' + c \bar{y}  = f$ sostituisco quello che ho trovato sopra a questa espressione:

\[
    \textbf{a}[c_1'(x) y_1'(x) + c_1(x) y_1''(x) + c_2'(x) y_2'(x) +c_2(x) y_2''(x)]+ \textbf{b}[c_1(x) y_1'(x) + c_2(x) y_2'(x)]+ \textbf{c}[c_1(x) y_1(x) + c_2(x) y_2(x)] = f(x)
\]

Adesso raccolgo a fattore comune le $c_i$:

\[
    c_1(x) [ \underbrace{a y_1''(x) +b y_1'(x) + c y_1(x)}_\text{=0}] + c_2(x) [\underbrace{a y_2''(x) + b y_2'(x) + c y_2(x)}_\text{=0}]+ a [c_1'(x) y_1'(x)+ c_2'(x) y_2'(x)] = f(x)
\]

quindi mi rimane:

\[
    c_1'(x) y_1'(x) + c_2'(x) y_2'(x) = \frac{f(x)}{a}
\]

Ottengo il sistema di 2 equazioni nelle due incognite ($c_1'(x),c_2'(x)$) non omogeneo:

    \begin{equation}
        \begin{cases}
            c_1'(x)y_1(x) + c_2'(x) y_2(x) = 0\\
            c_1'(x) y_1'(x) + c_2'(x) y_2'(x)  = \frac{f(x)}{a}
        \end{cases}\,.
    \end{equation}

La matrice dei coefficienti del sistema e':

\[
\begin{bmatrix}
y_1(x) & y_2(x) \\
y_1'(x) & y_2'(x) \\
\end{bmatrix}
\neq 0
\]

e' la matrice Wronskiana.

Uso il metodo di Cramer per risolvere il sistema:

\[
    A = \begin{pmatrix}
        a_{11} & a_{12}  \\
        a_{21} & a_{22}  \\
\end{pmatrix}
\]

$det A = a_{11} a_{22} - a_{12} a_{21}$

Il metodo:

\[
c_1'(x) = 
    \frac{
\begin{vmatrix}
0 & y_2(x)  \\
\frac{f(x)}{a} & y_2'(x)  \\
\end{vmatrix}
    }{
\begin{vmatrix}
y_1(x) & y_2(x)  \\
y_1'(x) & y_2'(x)  \\
\end{vmatrix}
    }
\]

\[
    = \frac{- y_2(x) \frac{f(x)}{a}}{y_1(x) y_2'(x) - y_2(x) y_1'(x)}
\]

\[
c_2'(x) = 
    \frac{
\begin{vmatrix}
y_1(x) & 0  \\
y_1'(x) & \frac{f(x)}{a}  \\
\end{vmatrix}
    }{
\begin{vmatrix}
y_1(x) & y_2(x)  \\
y_1'(x) & y_2'(x)  \\
\end{vmatrix}
    }
\]

\[
    = \frac{- y_1(x) \frac{f(x)}{a}}{y_1(x) y_2'(x) - y_2(x) y_1'(x)}
\]

Ora dobbiamo integrare

\[
    c_1(x) = \int_{}^{} {c_1'(x)} \: dx 
\]

e

\[
    c_2(x) = \int_{}^{} {c_2'(x)} \: dx 
\]

Si ha che l'insieme delle soluzioni di (1):

\[
    y(t)  = \underbrace{c_1 y_1(x) + c_2 y_2(x)}_\text{generale} + \underbrace{c_1(x) y_1(x) + c_2 y_2(x)}_\text{particolare}
\]

Assegnando le condizioni iniziali: 

\[
    y(x_0) = y_0
\]

\[
    y'(x_0) = y_0'
\]

Quindi il problema di Cauchy mi viene:

    \begin{equation}
        \begin{cases}
            ay''+ by'+cy= f(x)\\
            y(x_0)=y_0\\
            y'(x_0) = y_0'
        \end{cases}\,.
    \end{equation}

Trovare le soluzioni dei seguenti problemi di Cauchy

\textbf{Esempio 1} 

    \begin{equation}
        \begin{cases}
            3y'' + 5y' + 2y=3e ^{2x}\\
            y(0) = 0\\
            y'(0) = 1
        \end{cases}\,.
    \end{equation}

Scriviamo (1) e (2):

1)
\[
    3y''+5y'+2y = 3 e^{2x}
\]

2)

\[
    3y''+5y'+2y = 0
\]

Risolvo:

\[
    3 \lambda ^{2} + 5 \lambda + 2 = 0
\]

$p = 6$ $s=5= 3+2$ 

\[
    3 \lambda^{2} + 3 \lambda + 2 \lambda + 2 =0
\]

\[
    3 \lambda ( \lambda+1) + 2( \lambda + 1) =0
\]

\[
    (3 \lambda +2 ) ( \lambda +1 ) =0
\]

\[
    \lambda= -\frac{2}{3}, \lambda=-1
\]

quindi ho soluzioni linearmente indipendenti di (2):

\[
    y_1(x) = e ^{-\frac{2}{3}x}, y_2(x) = e ^{-x}
\]

Integrale generale di (1):

\[
    y_0(x) = c_1 e^{-\frac{2}{3}x} + c_2 e ^{-x},c_1,c_2 \in \mathbb{R}
\]


Una soluzione particolare di (1) e' dunque:

\[
    \bar{y} (x) = A e ^{2x}
\]

Ora derivo due volte:

\[
    \bar{y} '(x) = 2A e ^{2x}
\]

\[
    \bar{y} ''(x) = 4A e ^{2x}
\]

sostituendo poi in (1):

\[
    12Ae ^{2x} + 10 A e ^{2x} + 2 A e ^{2x} = 3 e^{2x}
\]

\[
    24A e ^{2x} = 3 e ^{2x}
\]

\[
    A = \frac{1}{8}
\]

Adesso devo imporre le condizioni iniziali a queste due:

\[
    y(x) = c_1 e ^{-\frac{2}{3}x} + c_2 e ^{-x}+ \frac{1}{8}e ^{2x}
\]

\[
    y'(x) = -\frac{2}{3}c_1 e ^{-\frac{2}{3}x} - c_2 e ^{-x}+ \frac{1}{4} e ^{2x}
\]


    \begin{equation}
        \begin{cases}
            c_1+c_2+\frac{1}{8}=0\\
            -\frac{2}{3}c_1-c_2+\frac{1}{4}=1
        \end{cases}\,.
    \end{equation}

    \begin{equation}
        \begin{cases}
            c_1=c_2-\frac{1}{8}\\
            \frac{2}{3}c_2+\frac{1}{12}-c_2+\frac{1}{4}=1
        \end{cases}\,.
    \end{equation}


    \begin{equation}
        \begin{cases}
            c_1=c_2-\frac{1}{8}\\
            -\frac{1}{3}c_2= 1- \frac{1}{3}
        \end{cases}\,.
    \end{equation}

    \begin{equation}
        \begin{cases}
            c_1=c_2-\frac{1}{8}\\
            -\frac{1}{3}c_2=\frac{2}{3}
        \end{cases}\,.
    \end{equation}

    \begin{equation}
        \begin{cases}
            c_1=\frac{15}{8}\\
            c_2=-2
        \end{cases}\,.
    \end{equation}


Infine quindi la soluzione del problema:

\[
    y(x) = \frac{15}{8}e ^{-\frac{2}{3}x}- 2 e ^{-x}+ \frac{1}{8}e ^{2x}
\]


\textbf{Esempio 2} 

    \begin{equation}
        \begin{cases}
    y''(x) -2y'(x)+ y(x) = 5sinx \\
            y(0) = 0\\
            y'(0) = 1
        \end{cases}\,.
    \end{equation}


Risolvo l'equazione associata:

\[
    \lambda^{2}-2 \lambda + 1=0
\]

\[
    (\lambda-1)^{2}=0
\]

quindi $\lambda_1=\lambda_2=1$


Devo trovare:

\[
    y_0(x) = c_1 e ^{x} + c_2 x e ^{x},c_1,c_2 \in \mathbb{R}
\]

la soluzione particolare e' per essere sicuri di prendere il termine noto che e' un seno:

\[
    \bar{y} (x) = A cosx + B sinx
\]

le derivate:

\[
    \bar{y} '(x)  = -A sinx + B cosx
\]

\[
    \bar{y} ''(x)  = -A cosx - B sinx
\]
    

Sostituiamo a quella iniziale:

\[
    \cancel{-A cosx} \cancel{- Bsinx} + 2A sinx - 2B cosx+ \cancel{A cosx} + \cancel{Bsinx} = 5sinx
\]

e quindi $2A = 5$ e $B=0$:

\[
    \bar{y} (x) = \frac{5}{2} cosx
\]

adesso devo trovare:

\[
    y(x) = c_1 e ^{x}+ c_2 x e ^{x} + \frac{5}{2} cosx
\]

\[
    y'(x)  = c_1 e ^{x} + c_2 e ^{x} + c_2x e ^{x} - \frac{5}{2}sinx
\]

Adesso impongo le condizioni iniziali:

    \begin{equation}
        \begin{cases}
            c_1 +\frac{5}{2}=0\\
            c_1+c_2= 1
        \end{cases}\,.
    \end{equation}

    \begin{equation}
        \begin{cases}
            c_1 = -\frac{5}{2}\\
            c_2 = \frac{7}{2}
        \end{cases}\,.
    \end{equation}

La soluzione del problema quindi:

\[
    y(x) = -\frac{5}{2}e ^{x}+ \frac{7}{2} x e ^{x} + \frac{5}{2} cosx
\]

\textbf{Esempio 3} 

    \begin{equation}
        \begin{cases}
            y''+ 2y'+ 2y= 3x^{2}\\
            y(0) = 0\\
            y'(0) = 1
        \end{cases}\,.
    \end{equation}


risolvo:

\[
    \lambda^{2}+2 \lambda+2 =0
\]

\[
    \lambda= \frac{-1 \pm \sqrt{1-2}}{1}= -1 \pm 1
\]

le soluzioni sono complesse:

\[
    \alpha \pm  i \beta
\]

con $\alpha = -1$ e $\beta = 1$ quindi sostituisco:

\[
    y_0(x) = y_0(c_1,c_2) = e ^{-x}(c_1cosx + c_2 sinx)
\]

la soluzione particolare:

\[
    \bar{y} (x) = A x^{2}+Bx+C
\]

derivo due volte:

\[
    \bar{y} '(x) = 2Ax + B
\]

\[
    \bar{y} ''(x) = 2A
\]

sostituisco in (1):

\[
    2A + 4Ax + 2B + 2Ax^{2}+2Bx + 2C = 3x^{2}
\]

\[
    2Ax^{2} + 2(2A+B) x + 2(A+B+C) = 3x^{2}
\]

risolvo un sistema per le ingnite $A,B,C$ e quindi trovo che:

    \begin{equation}
        \begin{cases}
            2A = 3\\
            2A + B = 0\\
            A+B+C= 0
        \end{cases}\,.
    \end{equation}

    \begin{equation}
        \begin{cases}
            A=\frac{3}{2}\\
            B=-3\\
            \frac{3}{2}-3+C=0
        \end{cases}\,.
    \end{equation}

    \begin{equation}
        \begin{cases}
            A=\frac{3}{2}\\
            B=-3\\
            C=\frac{3}{2}
        \end{cases}\,.
    \end{equation}


Quindi la soluzione particolare:

\[
    \bar{y} (x) = \frac{3}{2}x^{2}-3x+\frac{3}{2}
\]


l'espressione quindi e':

\[
    y(x) = e ^{-x}(c_1 cosx + c_2 sinx ) + \frac{3}{2}x^{2}-3x+\frac{3}{2}
\]

derivo:

\[
    y'(x) = e ^{-x}(c_1 cosx+ c_2 sinx) + e ^{-x}(-c_1sinx+c_2 cosx) +3x -3
\]

quindi impongo le condizioni:

    \begin{equation}
        \begin{cases}
            c_1 +\frac{3}{2}=0\\
            -c_1+c_2 -3 = 1
        \end{cases}\,.
    \end{equation}

    \begin{equation}
        \begin{cases}
            c_1=-\frac{3}{2}\\
            c_2=-\frac{5}{2}
        \end{cases}\,.
    \end{equation}

Quindi la soluzione del problema:

\[
    y(x) = e ^{-x}(-\frac{3}{2}cosx + \frac{5}{2} sinx) + \frac{3}{2}x^{2}-3x + \frac{3}{2}
\]

\textbf{Esempio 4} 

    \begin{equation}
        \begin{cases}
            y''-6y' + 9y = e ^{3x}\\
            y(0) = 1\\
            y'(0) = 0
        \end{cases}\,.
    \end{equation}

\[
    \lambda^{2} -6 \lambda + 9 = 0
\]

\[
    \lambda_1=\lambda_2=3
\]

\[
    y_0(x)  = c_1 e ^{3x}+ c_2 x e ^{3x}
\]

qua abbiamo il problema che $\lambda=3$ che e' la stessa del termine noto, devo quindi modificarla:

\[
    \bar{y} (x) = A x^{2}e ^{3x}
\]

dove il due della x viene dalla molteplicita', 2 in questo caso.

Proviamo a risolverlo con il metodo di variazione delle costanti:

\[
    \bar{y} (x) = c_1(x) e ^{3x}+ c_2(x) x e^{3x}
\]

lo risolvo col sistema (porco dio):

    \begin{equation}
        \begin{cases}
    c_1'(x)e ^{3x} + c_2'(x) x e ^{3x}=0 \\
    c_1'(x) 3 e ^{3x} + c_2'(x) (e ^{3x} + 3x e ^{3x}) = e ^{3x}
        \end{cases}\,.
    \end{equation}

% \[
%     c_1'(x) = \frac{
%                 \begin{vmatrix}
% 0 & x e ^{3x}  \\
%  e ^{3x} & (3x+1) e ^{3x}  \\
% \end{vmatrix}
%     }
%         \begin{vmatrix}
%     e ^{3x} & x e ^{3x}  \\
%     3 e ^{3x} & (3x+1) e ^{3x}  \\
%         \end{vmatrix}
% \]

facnedolo viene:

    \begin{equation}
        \begin{cases}
            c_1(x) = \int_{}^{} {-x} \: dx =-\frac{x^{2}}{2}\\
            c_2(x) = \int_{}^{} {} \: dx  = x
        \end{cases}\,.
    \end{equation}

\[
    \bar{y} (x) = - \frac{x^{2}}{2}e ^{3x} + x x e ^{3x} = \frac{x^{2}}{2}e ^{3x}
\]


\textbf{Risoluzione esercizio 6 appunti prof} 

    \begin{equation}
        \begin{cases}
            y'= \frac{sinx}{cosy}\\
            y( \frac{\pi}{2} = \frac{\pi}{6}
        \end{cases}\,.
    \end{equation}

questa e' a variabili separabili:

\[
    y'(x) cosy(x) = sinx
\]

\[
    \int_{}^{} {y'(x) cosy(x)} \: dx = \int_{}^{} {sinx} \: dx 
\]

\[
    \int_{}^{} {cosy} \: dy = \int_{}^{} {sinx} \: dx 
\]

\[
    sin y = -cos x +c
\]

impongo adesso le condiizoni:

\[
    sin y( \frac{\pi}{2}) = - cos \frac{\pi}{2} + c
\]

\[
    sin \frac{\pi}{6} = c
\]

quindi:

\[
    \frac{1}{2}=c
\]

quindi sostituisco la c:

\[
    sin y(x) = - cos x + \frac{1}{2}
\]

Devo fare l'arcosin e vedo dove e' definita la cosa:

\[
    -1 \le sin y(x) \le 1
\]

ma anche:

\[
    -1 \le cosx + \frac{1}{2} \le 1
\]

impongo il sistema:

    \begin{equation}
        \begin{cases}
            -cosx +\frac{1}{2}\ge -1\\
            -cos x + \frac{1}{2} \le 1
        \end{cases}\,.
    \end{equation}

    \begin{equation}
        \begin{cases}
            cosx \le  \frac{3}{2}\\
            cosx \ge -\frac{1}{2}
        \end{cases}\,.
    \end{equation}

% \begin{tikzpicture}
% \begin{axis}[
%     xmin = -5, xmax = 5,
%     ymin = -5, ymax = 5]
%     \addplot[
%         domain = -5:5,
%         samples = 200,
%         smooth,
%         thick,
%         blue,
%     ] {sinx};
% \end{axis}
% \end{tikzpicture}

Prendo la soluzione nell'intervallo:

\[
    -\frac{2}{3}\pi \le  x \le  \frac{2}{3}\pi
\]

la soluzione del problema: 

\[
    y(x) = arcsin(-cosx +\frac{1}{2})
\]
   
\newpage

\section{Lezione 7}

\subsection{Funzioni in piu variabili}

In particolare:

\begin{itemize}
    \item funzioni reali di piu variabili $f: A \in R^{n} \rightarrow \mathbb{R}$
    \item funzioni a valori vettoriali $g: A \in \mathbb{R}^{n}\rightarrow \mathbb{R}^{m}$
\end{itemize}

\defn{Vettore}{ Il vettore $n \in \mathbb{R}^{n}$ e' una n-pla $x=(x_1,x_2,...,x_n)$ }



Operazioni in $\mathbb{R}^{n}$:

\begin{itemize}
    \item moltiplicazione per scalare

        \[
            \forall c \in \mathbb{R}, \forall \textbf{x} \in \mathbb{R}^{n}
        \]

        \[
            c \textbf{x} = (cx_1,...,cx_n)
        \]

    \item somma

        \[
            \forall \textbf{x}, \textbf{y}  \in \mathbb{R}^{n}
        \]

        \[
       \textbf{x}  =(x_1,...,x_n), \textbf{y}  = (y_1,...,y_n)
        \]

        \[
            \textbf{x}+ \textbf{y} = (x_1+y_1,...,x_n+y_n)
        \]

    \item prodotto scalare

        \[
            \forall \textbf{x} ,\textbf{y} \in \mathbb{R}^{n}
        \]

        \[
            \langle x,y \rangle  = x \bullet y := \sum^{n}_{i=1} x_i y_i
        \]

        L'operazione quindi va $\langle , \rangle : \mathbb{R}^{n} \times \mathbb{R}^{n} \rightarrow \mathbb{R}$

        \textbf{Esempio} 

        \[
            x=(1,2,0,3,5) \in \mathbb{R}^{5}
        \]

        \[
            y=(2,5,1,7,3) \in \mathbb{R}^{5}
        \]

        \[
            \langle x,y \rangle  = x \bullet y = 48
        \]

        Il prodotto scalare verifica le seguenti proprita'

        \begin{enumerate}
            \item \textbf{Bilinearita'} (lineare su ogni fattore):

                \[
                    (\alpha x_1 + \beta x_2) \bullet y = \langle (\alpha x_1 + \beta x_2), y \rangle  = \alpha \langle x_1,y \rangle+ \beta \langle x_2,y \rangle = \alpha x_1 \bullet y + \beta x_2 \bullet y
                \]

                \[
                    \forall x_1,x_2,y \in \mathbb{R}, \forall \alpha, \beta \in \mathbb{R}
                \]

            \item \textbf{Simmetria} (l'ordine non conta)

                \[
                    \forall x,y \in \mathbb{R}^{n}
                \]

                \[
                    x \bullet y = y \bullet x
                \]

            \item \textbf{Positivita'} 
                
                \[
                    \forall x \in  \mathbb{R}^{n}
                \]

                \[
                    x=(x_1,...,x_n)
                \]

                \[
                    x \bullet x = \langle x,x \rangle = \sum^{n}_{i=1} x_i^{2}\ge 0
                \]

                \[
                    x \bullet x = \langle x,x \rangle = 0 \Leftrightarrow x = 0 = (0,...,0)\ \text{vettore nullo}
                \]

        \end{enumerate}


        \defn{Norma}{
        Il numero reale (non negativo)

        \[
          |x|:=  \sqrt{x \bullet x} = \sqrt{\langle x,x \rangle}
        \]


        si chiama \textbf{lunghezza} o \textbf{norma} del vettore

        }

           
\end{itemize}

\newpage

  % proposizione 
\proposizione{Formula di Carnot}{

    \[
        \forall x,y \in \mathbb{R}^{n}
    \]

    si ha:

    \[
    |x+y| = |x|^{2} + |y|^{2} + 2x\bullet y
    \]
}

\begin{proof}
       \[
           |x+ y| ^{2} = \langle x+y , x+y \rangle = (x+y) \bullet (x+y) \overset{\text{bilinearita'}}{=} \langle x,x+y \rangle + \langle y,x+y \rangle \overset{\text{sempre bilinearita'}}{=} 
       \]    

       \[
           =\langle x,x \rangle + \langle x,y \rangle + \langle y,x \rangle + \langle y,y \rangle = \langle x,x \rangle + 2\langle x,y \rangle + \langle y,y \rangle= 
       \]

       \[
            = |x|^{2} + |y|^{2} + 2x \bullet y
       \]
\end{proof}


Altra cosa interessante:

 \[
    |x+y|^{2}= |x|^{2} + |y|^{2} \Leftrightarrow x\bullet y  =0
 \]



 \proposizione{Disuguaglianza di Cauchy-Schwarz}{

     \[
         \forall x,y \in \mathbb{R}^{n}
     \]

     \[
         |\langle x,y \rangle| \le |x| |y|
     \]

     si ha:

     \[
         x\bullet y = |x| |y|  \Leftrightarrow y = 0 \lor x=\lambda y \text{ con } \lambda \in \mathbb{R}\ge 0
     \]

 }

 \begin{proof}
     Se
     \[
        y=0
     \]

     \[
         y=0=(0,...,0) 
     \]
     
     questo caso va bene.

     Sia dunque $\mathbb{R}^{n} \rightarrow y \neq 0$ e consideriamo la funzione reale di una variabile reale $t \rightarrow |x+ty|^{2}\ge 0$ polinomio di secondo grado in t

     \[
         |x + ty| ^{2} \overset{\text{Carnot}}{=} |x|^{2} + |ty|^{2} + 2\langle x,ty \rangle  = |x|^{2} + |y|^{2}t^{2} + 2 \langle x,y \rangle t
     \]

     e' un polinomio di II grado in t dove $|y|^{2}> 0 $ essendo $y \neq 0$

     Il nostro $\frac{\Delta}{4}$ deve essere non positivo:

     \[
         (x\bullet y ) ^{2} - |x| ^{2}|y|^{2} \le 0
     \]

     \[
         (x\bullet y ) ^{2}\le  |x| ^{2}|y|^{2} 
     \]

     da cui si ha la tesi.

     Si verifica, se si ha che

     \[
         \langle x,y \rangle = |x| |y|
     \]

     si ha che il $\Delta$ del trinomio di II grado e' nullo e dunque $t \in \mathbb{R}$ per cui $|x+ty|^{2}=0$ ovvero $x +ty=0$ $\rightarrow x=-ty$

     devo mostrare che $-t \ge 0$


     \[
         t = - \frac{\langle x,y \rangle}{|y|^{2}}
     \]

     si ricorda che $|y|>0$ essendo y non nullo

     \[
         -t = \frac{|x| |y|}{|y|^{2}}\ge 0
     \]

 \end{proof}


 Definiamo ora la funzione \textbf{lunghezza} che e' una norma $\mathbb{R}^{n}\rightarrow \mathbb{R}_0^{+}$

 C'e' una proprieta che e' quella di omogeneita':

 \[
     |\lambda x| = \underbrace{|\lambda|}_\text{valore assoluto} |x| 
 \]

 \[
 \forall \lambda \in \mathbb{R},x \in \mathbb{R}^{n}    
 \]

 ed anche 

 \defn{Disuguaglianza triangolare}{
 La disugualgianza triangolare si definisce come:

 \[
     |x+y| \le |x|+|y|
 \]
   
se $|x+y| = |x| + |y|$ $\rightarrow $ $y =0$ $\lor$ $x= \lambda y $ con $\lambda \ge 0$:

 }

 

\begin{proof}
       Dimostriamo la disuguaglianza triangolare, considero:

       \[
           |x+y|^{2} = \langle x+y , x+y \rangle = |x|^{2}+|y|^{2} + 2 \langle x,y \rangle \le 
       \]

       \[
           \le |x|^{2} + |y| ^{2} + 2|\langle x,y \rangle| \le \underbrace{|x| ^{2} + |y|^{2} + 2|x|\bullet |y|}_{(|x|+|y|)^{2}}
       \]

       estraendo e passando alle radici si ha

       \[
           |x+y| \le |x|+|y|
       \]
\end{proof}



\defn{Distanza Euclidea}{
Distanza Euclidea si definisce come $d(x,y)$:

\[
    d(x,y) := |x-y| = \sqrt{\langle x-y,x-y \rangle} = \sqrt{\sum^{n}_{i=1} (x_i - y_i)^{2}} \ge 0
\]

questa e' la norma

}

\newpage

\section{Lezione 8}

La distanza euclidea verifica le proprieta' (le stesse del valore assoluto):

\begin{enumerate}
    \item  $d(x,y) \ge 0, d(x,y) \in \mathbb{R}$
    \item  $d(x,y) = 0 \Leftrightarrow x=y$
    \item  $d(x,y) = d(y,x)$ simmetria
    \item  $d(x,y) \le d(x,z) + d(z,y), \forall x,y,z \in \mathbb{R}^{n} $ disugualgianza triangolare
\end{enumerate}

\defn{Spazio metrico}{ Uno spazio metrico e' un insieme X dotato di un'applicazione definita: $X \times X \rightarrow \mathbb{R}$ che verifica la proprieta' sopra:

    \[
        (\mathbb{R}^{n},\underbrace{d}_\text{distanza euclidea}) \text{ spazio metrico}
    \]

    esistono altre distanze che ci definiscono relative metriche equivalenti

}


\textbf{Esempio} 

\[
    \mathbb{R}^{2},\forall x,y \in \mathbb{R}^{2}
\]

\[
    x=(x_1,x_2),y=(y_1,y_2)
\]

\[
    d(x,y) = \sqrt{(x_1-y_1)^{2}+(x_2-y_2)^{2}}
\]

la distanza si puo scrivere anche:
   
\[
    d_1(x,y) = |x_2-x_1| + |y_2-y_1| \text{ dove abbiamo usato i valori assoluti}
\]

\subsection{Successioni convergenti in $\mathbb{R}^{n}$}

\defn{Successione}{ Una successione e' un elenco ordinato di numeri $\{x_n\} \subset \mathbb{R}^{n}$ (gli elementi della successione sono elementi di $\mathbb{R}^{n}$ ovvero n-plue di reali)

    \[
        x_n= (x_n^{1}, x_n^{2}, ..., x_n ^{n})
    \]

    si dice che converge a $x \in \mathbb{R}^{n}$ se:

    \[
    d(x_n,x) \xrightarrow[] {n \rightarrow +\infty} 0
    \]

    cioe':

    \[
        \forall \varepsilon >0 , \exists \bar{N} \in \mathbb{N}
    \]

    \[
        d(x_n, x ) < \varepsilon, \forall n \ge \bar{N} 
    \]

    \[
        |x_n - x| = \sqrt{(x_n^{1}-x^{1})^{2}+ ... +(x_n^{n}-x^{n})^{2}}
    \]

}

\proposizione{}{ Sia $\{x_n\} \subset \mathbb{R}^{n}$ una successione in $\mathbb{R}^{n}$. Si ha:

    \begin{enumerate}
        \item (\textbf{Unicita}) $\{x_n\}$ ha massimo un unico limite ( se $\{x_n\}$ ammette limite questo e' unico)
        \item (\textbf{Limitatezza}) ogni successione convergente e' limitata: 

             \[
                 \exists x_0 \in \mathbb{R}^{n},M \in \mathbb{R}
             \]

             \[
                 d(x_n,x_0) \le M, \forall n
             \]

         \item (\textbf{Sottosuccessione}) Se $\{x_n\}$ convergente a $x \in  \mathbb{R}^{n}$ ( o in $X$) allora ogni sottosuccessione ${x_{n_k}}$ estratta da ${x_n}$ converge allo stesso limite
    \end{enumerate}

}

\subsection{Elementi di topologia in $\mathbb{R}^{n}$ (in X)}

Sia $x_0 \in \mathbb{R}^{n}$ fissato e $r>0$

Si ha la seguente definizione

\defn{}{Si definisce palla aperta, disco aperto, intorno sferico di centro $x_0$ e raggio $r$ l'insieme e che si indica con $B(x_0,r)$ l'insieme:

    \[
        B(x_0,r) := \{ x \in \mathbb{R}^{n}, d(x,x_0)<r\} \subset \mathbb{R}^{n}
    \]

    praticamente un intorno di $x_0$ in $\mathbb{R}^{n}$


}

\begin{figure}[ht]
    \centering
    \incfig{disco-aperto}
    \caption{disco aperto}
    \label{fig:disco-aperto}
\end{figure}

\textbf{Esempio in $(\mathbb{R}^{2},d)$ e $(\mathbb{R}^{2},d_1)$} 

\[
    d(x,y) = \sqrt{\sum^{r}_{i=1} (x_i-y_i)^{2}}
\]

\[
    d_1(x,y) = |x_1-y_1| + |x_2-y_2|
\]

se $x_0=0$ e $r=1$

\[
    B(0,1)  = \{\bar{x} \in \mathbb{R}^{2}, d(x,0) <1\} = \{x \in  \mathbb{R}^{2}, \sqrt{x_1^{2}+y_1^{2}}<1\}
\]

questo e' un cerchio


\[
    B(0,1)  = \{\bar{x} \in \mathbb{R}^{2}, d_1(x,0) <1\} = \{x \in  \mathbb{R}^{2}, \underbrace{|x_1-0|}_{x_1} + \underbrace{|y_1-0|}_{y_1}<1\}
\]

questo e' un rombo (parallelogramma).


Abbiamo parlato di questa roba per definire i limiti attraverso gli intorni sferici

Quindi:

\[
\{ x_n\} \subset X \text{ converge a } x_0 \in X \Leftrightarrow \forall \varepsilon >0 , \exists n_\varepsilon, x_n \in B(x_0,\varepsilon), \forall n \ge n_\varepsilon
\]

\defn{Sottoinsieme limitato}{$A \subset X$ si dice limitato se esiste una palla aperta in cui $A$ risulta interamente contenuto:

    \[
        \exists r>0,\exists x_0 \in X \text{ t.c. } A \subset B(x_0,r)
    \]
}

\defn{Punto interno}{

Un punto di $x_0 \in X$ si dice interno ad $A$ dove $A \subset X$ e non solo $x_0 \in A$ ma esiste (almeno) un suo intorno sferico interamente contenuto in $A$:

\[
    \exists r>0\ B(x_0,r) \subset A
\]

$\dot A$ insieme di punti interni ad $A$

}

\defn{Punto esterno}{ $x_0 \in X$ si dice esterno ad $A$ ($A \subset X$) se non solo $x_0$ non appartiene ad $A$ ma vi e' almeno un suo intorno sferico completamente disgiunto ad $A$


    \[
        x_0 \in A \text{ e } \exists r>0\ B(x_0,r)\cap A = \emptyset
    \]

}


I punti che non sono ne eseterni ne interni si dicono di frontiera.


\defn{Insieme aperto}{$A \subset X$ si dice aperto se $A = \emptyset$ oppure se ogni punto e' un punto interno di $A$ (ovvero per ogni punto di $A$ c'e' un intorno sferico tutto contenuto in $A$}


\defn{Insieme chiuso}{ Un insieme $C \subset X$ si chiuso se il suo complementare e' un insieme aperto:

    \[
        X \setminus  C = A
    \]

}


\textbf{Esempio} 

Sia

\[
    A_2= \{ (x,y) \in \mathbb{R}^{2}, x \neq y^{2}\} \subset \mathbb{R}^{2}
\]

\begin{figure}[ht]
    \centering
    \incfig{esercizio-su-insieme-chiuso-1}
    \caption{esercizio su insieme chiuso 1}
    \label{fig:esercizio-su-insieme-chiuso-1}
\end{figure}


\textbf{Esercizio per casa} 

\[
    A_1= \{ (x,y) \in \mathbb{R}^{2}, x^{2}+ y^{2} < 25\}
\]

\[
    A_2= \{ (x,y) \in \mathbb{R}^{2}, x^{2}+ 2y +1  \le  0\}
\]


\section{Lezione 9}

\subsection{Limiti di Funzioni in piu variabili}

\defn{Punto di accumulazione}{

    Un punto $x_0 \in R^{n}$ di accumulazione per $A \subseteq R^{n}$ si dice punto di accumulazione se in ogni intorno circolare di $x_0$ c'e' almeno un punto di $A$ diverso da $x_0$

}


\textbf{Esempi} 

\begin{itemize}
    \item I punti che costituiscono l'insieme dei punti interni di $A: \dot A$  sono punti di accumulazione
    \item I punti di frontiera, ovvero i punti di $\delta A$ possono essere punti di accumulazione di $A$ oppure non esserlo in quest'ultimo caso si dice che e' un punto isolato
\end{itemize}

\newpage

\defn{Convergenza in $\mathbb{R}^{n}$}{Data una successione $ \{x_n\}\in \mathbb{R}^{n}$ questa si dice che converge a $x_0 \in  \mathbb{R}^{n}$ se:

\[
    \lim_{ n \to +\infty } d(x_n,x_0)=0
\]

questo equivale a dire $\forall \varepsilon >0, \exists \bar{N} \in \mathbb{N}$ e $\forall n \ge \bar{N}$ si ha:

\[
    d(x_n,x_0) < \varepsilon
\]

}

\defn{Punto di accumulazione con limiti}{

    $x_0$ e' di accumulazione per $A$ $\Leftrightarrow x_0$ e' il limite di una successione di elementi di $A$ tutti diversi da $x_0$
}


\textbf{Esempio di punti di accumulazione} 

\[
    A \{\bar{x}  \in R^{2};1<\underbrace{|x|}_\text{d(x,0)}<2\}
\]


\begin{figure}[ht]
    \centering
    \incfig{disegno-punto-di-accumulazione-esempio-1}
    \caption{disegno punto di accumulazione esempio 1}
    \label{fig:disegno-punto-di-accumulazione-esempio-1}
\end{figure}


Tutti i punti di $A \in \mathbb{R}^{2}$ sono punti di accumulaione

I punti del disegno sono sia punti di frontiera che di accumulazione


Nel caso di $\emptyset, \mathbb{R}^{n}$ gli insiemi sono contemporaneamente sia aperti che chiusi.


\defn{Chiusura di un insieme $A \subset \mathbb{R}^{n}$}{

    Si indica con $\bar{A} $ e' un sottoinsieme di $\mathbb{R}^{n}$ dato dall'unione di $A$ e dei suoi punti di accumulazione ($DA$)

    $\bar{A} $ e' un insieme chiuso. Lo si puo' pensare come l'intersezione dei chiusi contenenti $A$.

    Si puo' inoltre dimostrare che:

    \[
        \bar{A} = A \cup \delta A
    \]

}

\defn{Dominio}{ Un dominio $D$ in $\mathbb{R}^{n}$ e' la chiusura di un insieme aperto:

    \[
        D = \bar{A}  = A \cup \delta A
    \]
}

Consideriamo ora le funzioni in piu' variabili $f: A \subset \mathbb{R}^{n} \rightarrow \mathbb{R}$

\defn{Limite di funzioni in piu variabili}{

Sia $x_0 \in \mathbb{R}^{n}$ un punto di accumulazione per $A$

Si dice che $f(\bar{x})$ tende (ha limite) a $l$ per $\bar{x} $ che tende a $x_0$:

\[
    \lim_{ \bar{x}  \to x_0 } f(\bar{x} ) = l
\]

scrivendolo tramite gli intorni: se $\forall $ intorno $U \subset \mathbb{R}$ di $l$ esiste un intorno di $x_0$ (sferico) $I(x_0,r)$ con $r>0$ 

tale che $f(\bar{x}) \in U$ $\forall x \in \underbrace{I(x_0,r)}_{B(x_0,r)}\cap (A\{x_0\})$ 

L'altra definizione con i delta:

\[
    \forall \varepsilon>0\  \exists \delta >0
\]

tale che 

\[
    \underbrace{|f(x) - l|}_{d(f(x),l) \in \mathbb{R}} <\varepsilon
\]

$\forall \bar{x} \in A \setminus \{x_0\}$ con $|x-x_0| < \delta$
}

\subsection{Proprieta dei limiti di funzioni in piu variabili}

Adesso parliamo di un po' di proprieta':

\begin{itemize}
    \item Il limite quando esiste e' \textbf{unico}
    \item I limiti di \textbf{somme} e di \textbf{prodotti} di funzioni sono dati dalla somma e dal prodotto dei limiti (se definito)
    \item Il limite del \textbf{quoziente} di due funzioni e' il quoziente dei limiti (se definito)
\end{itemize}

\textbf{Esempi} 

Sia 

\[
    f(x,y) = \frac{x^{2}}{\sqrt{x^{2}+y^{2}}}
\]

La mia $f: \underbrace{\mathbb{R}^{2}\setminus \{(0,0)\}}_\text{A aperto}\rightarrow \mathbb{R}$

Il punto $(0,0)$ e' punto di accumulazione per $A$ 

Vogliamo vedere che succede quando la funzione tente a questo punto di accumulazione:

\[
    \lim_{ (x,y) \to (0,0) } f(x,y) = 0
\]

questo significa che $\forall \varepsilon >0 , \exists \delta >0$ tale che $|f(x,y) -0| = |f(x,y)| <\varepsilon$ $\forall (x,y) \in A = \mathbb{R}^{2}\setminus \{(0,0)\}$ con $0< \sqrt{x^{2}+y^{2}}<\delta$:

\[
    \underbrace{0\le}_\text{sempre positiva} f(x,y) = \frac{x^{2}}{\sqrt{x^{2}+y^{2}}} \le \frac{x^{2}+y^{2}}{\sqrt{x^{2}+y^{2}}} \overset{\text{razionalizzo}}{=} \sqrt{x^{2}+y^{2}}
\]

E dunque  $\forall \varepsilon$ si ha:

\[
    0 \le f(x,y) < \varepsilon
\]

per ogni $(x,y) \neq (0,0)$ t.c. $\sqrt{x^{2}+y^{2}} < \varepsilon$


\textbf{Esercizio per casa}

Mostrare che 

\[
    \lim_{ (x,y) \to (0,0) } \frac{x}{\sqrt{x^{2}+y^{2}}} \text{ non esiste}
\] 

\textbf{Soluzione} 

\begin{figure}[ht]
    \centering
    \incfig{esercizio-limite-casa}
    \caption{esercizio limite casa}
    \label{fig:esercizio-limite-casa}
\end{figure}

Se calcolo la funzione:

\[
    f(x,0) = \frac{x}{\sqrt{x^{2}}}= \frac{x}{|x|}
\]

questa fa:

\begin{equation}
    \begin{cases}
           1,x>0\\
           -1,x<0
    \end{cases}\,.
\end{equation}

Il limite quindi non esiste perche ha valori diversi a seconda del caso e non va bene

Invece:

\[
    f(0,y) = 0
\]

\section{Lezione 10}

\proposizione{}{Se 

    \[
        \lim_{ (x,y) \to (x_0,y_0) } f(x,y)=l
    \]

    allora per ogni sottoinsieme $C$ di $A$ (si sottindende che $P_0=(x_0,y_0)$ sia punto di accumulazione per $C$)

    Si deve avere:

    \[
        \lim_{ \underbrace{(x,y) \to (x_0,y_0)}_{(x,y) \in C} } f(x,y)=l
    \]

}

\textbf{Esercizi} 

\textbf{1} 

Mostraiamo che:

\[
    \lim_{ (x,y) \to (0,0) } \frac{xy}{x^{2}+y^{2}}
\]

non esiste.

Restringiamo lo studio di funzione lungo l'asse x ($y=0$):

\[
    f(x,0) = 0
\]

\[
    \lim_{ \underbrace{(x,y) \to (0,0)}_{y=0} } f(x,y)
\]

Stessa cosa lungo l'asse y ($x=0$):

\[
    \lim_{ \underbrace{(x,y) \to (0,0)}_{x=0} } f(x,y)
\]

Candidato limite e' a 0 


Adesso ci spostiamo con altri parametri tipo la bisettrice del primo e del terzo quadrante $y=x$:

\[
    \lim_{ \underbrace{(x,y) \to (0,0)}_{y=x} } f(x,y) = \lim_{ x \to 0 } f(x,x)=
\]


\[
    = \lim_{ x \to 0 } \frac{x^{2}}{x^{2}+y^{2}} = \lim_{ x \to 0 } \frac{x^{2}}{2x^{2}}= \frac{1}{2}
\]

\textbf{2} 

\[
    \lim_{ (x,y) \to (0,0) } \frac{xy^{2}}{x^{2}+y^{4}}
\]

$f:\mathbb{R}^{2}\setminus (0,0) \rightarrow \mathbb{R}$

per $y=0$ viene a 0 ed anche per $x=0$

Considero quindi qualunque retta passante per l'origine:

\[
    y= mx
\]

con $m \neq 0$ e $x \neq 0$:

\[
    \lim_{ \underbrace{(x,y) \to (0,0)}_{y=mx} } f(x,y) = \lim_{ x \to 0 } f(x,mx) = \lim_{ x \to 0 } \frac{m^{2}x^{3}}{x^{2}+m^{4}x^{4}}= 0
\]

ma questo non basta, devo controllare anche il caso della parabola $y^{2}=x$:


\[
    \lim_{ \underbrace{(x,y) \to (0,0)}_{x=y^{2}} } f(x,y) = \lim_{ y \to 0 } f(y^{2},y) = \lim_{ y \to 0 } \frac{y^{4}}{y^{4}+y^{4}} = \lim_{ y \to 0 } \frac{y^{4}}{2y^{4}} = \frac{1}{2} \neq 0
\]

\defn{Funzione continua in piu variabili}{ Sia una funzione e sia $P_0$ un punto di accumulazione per a, si dice che la funzione e' continua in $P_0$ se:

    \[
        \lim_{ P \to P_0 } f(P) = f(P_0)
    \]

    se $P_0$ e' un punto isolato per $A$ per convenzione $f$ e' continua

}

\textbf{Esempi} 

Avendo queste due funzioni 

\[
    f(x,y)=x
\]

\[
    g(x,y) = y
\]

devo mostrare che $f$ e $g$ sono continue in ogni punto:

Consideriamo la $f$
 
Sia dunque $(x_0,y_0) \in  \mathbb{R}^{2}$ e $\varepsilon >0$ dobbiamo mostrare che $\exists \delta= \delta(\varepsilon) >0$:

\[
     d(f(x,y)- f(x_0,y_0) ) < \varepsilon
\]

se $d(P,P_0) < \delta$ 

scritto meglio

\[
    d(P,P_0) = \sqrt{(x-x_0)^{2}+(y-y_0)^{2}}
\]

mostriamo che $\sqrt{(x-x_0)^{2}+(y-y_0)^{2}} < \delta $ si ha $|x-x_0|< \varepsilon$:


\[
|x-x_0| = \sqrt{(x-x_0)^{2}}\le \sqrt{(x - x_0) ^{2} + (y- y_0) ^{2}} < \delta
\]

dobbiamo prendere quindi $ \delta =\varepsilon$


\teorema{}{Siano $f$ e $g$ continue (sugli opportuni domini) allora:

    \begin{itemize}
        \item $f+g , f\cdot g$ sono continue 
        \item se $g \neq 0$ allora $\frac{f}{g}$ e' continua
        \item se $g>0$ allora $f^{g}$ e' continua
        \item la funzione comporta $g \circ f$ e' continua (dove e' definita)
    \end{itemize}
}

Sono dunque funzioni continue:

\begin{itemize}
    \item I polinomi in due variabili 
    \item Le funzioni razionali (rapporti, quoziente di polinomi)
    \item Le funzioni elementari 
\end{itemize}


Condizione necessaria affinche $f(x,y)$ ammetta limite $l$ quando $(x,y) \rightarrow  (x_0,y_0)$ e' che per ogni curva regolare di equazione:

\begin{equation}
    \begin{cases}
           x=x(t)\\
           y=y(t)
    \end{cases}\,.
\end{equation}

questa e' una curva  passante per $P_0= (x_0,y_0)$:

\[
    \lim_{ t \to t_0 } f(x(t), y(t)) 
\]

si arriva alla stessa coclusione di non esistenza del limite se la restrizione di $f(x,y)$ ad una curva (come sopra) non ha limite. Ovviamente non e' vero il viceversa

\end{document}

